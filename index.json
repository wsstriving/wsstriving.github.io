[{"authors":["admin"],"categories":null,"content":"I obtained my Ph.D. degree in Shanghai Jiao Tong University in 2020.09, under the supervision of Kai Yu and Yanmin Qian. During the Ph.D. my research interests include deep learning based approaches for speaker recognition, speaker diarization and voice activity detection. After my graduation, I joined Tencent Games as a senior researcher, where I led a speech group and extended the research interest to speech synthesis, voice conversion, music generation and audio retrivial. Currently, I am with the SpeechLab in Shenzhen Research Institute of Big Data led by Haizhou Li and work closely with Zhizheng Wu.\nI serve as a regular reviewer for speech/deep learning related conferences/journals: Interspeech, ICASSP, ICME, TASLP and Neural Networks.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://wsstriving.github.io/author/shuai-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuai-wang/","section":"authors","summary":"I obtained my Ph.D. degree in Shanghai Jiao Tong University in 2020.09, under the supervision of Kai Yu and Yanmin Qian. During the Ph.D. my research interests include deep learning based approaches for speaker recognition, speaker diarization and voice activity detection.","tags":null,"title":"Shuai Wang","type":"authors"},{"authors":["Xintao Zhao","Shuai Wang","Yang Chao","Zhiyong Wu","Helen Meng"],"categories":["SEL"],"content":"","date":1683295919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683295919,"objectID":"541606d0d2d05474fa1ccbd115f1d62b","permalink":"https://wsstriving.github.io/publication/2023-icme-vc/","publishdate":"2022-05-05T22:11:59+08:00","relpermalink":"/publication/2023-icme-vc/","section":"publication","summary":"ICME 2023","tags":[],"title":"Adversarial Speaker Disentanglement Using Unannotated External Data for Self-supervised Representation Based Voice Conversion","type":"publication"},{"authors":["Jinchao Li","Shuai Wang","Yang Chao","Xunying Liu","Helen Meng"],"categories":["SEL"],"content":"","date":1655216039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655216039,"objectID":"b59aad468972bc7e3da9c2f468bd99fe","permalink":"https://wsstriving.github.io/publication/2022-is-emotion/","publishdate":"2022-06-14T22:13:59+08:00","relpermalink":"/publication/2022-is-emotion/","section":"publication","summary":"Interspeech 2022","tags":[],"title":"Context-aware Multimodal Fusion for Emotion Recognition","type":"publication"},{"authors":["Bei Liu","Zhengyang Chen","Shuai Wang","Haoyu Wang","Bing Han","Yanmin Qian"],"categories":["SEL"],"content":"","date":1655215919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655215919,"objectID":"b0d8a49f51d528cf75e4bdfec083daf2","permalink":"https://wsstriving.github.io/publication/2022-is-speaker/","publishdate":"2022-06-14T22:11:59+08:00","relpermalink":"/publication/2022-is-speaker/","section":"publication","summary":"Interspeech 2022","tags":[],"title":"DF-ResNet: Boosting Speaker Verification Performance with Depth-First Design","type":"publication"},{"authors":["Aiwen Deng","Shuai Wang","Wenxiong Kang","Feiqi Deng"],"categories":["SEL"],"content":"","date":1652537639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652537639,"objectID":"f643eaaeace9f11484aba3cb109a3687","permalink":"https://wsstriving.github.io/publication/2022-icassp-skd/","publishdate":"2022-05-14T22:13:59+08:00","relpermalink":"/publication/2022-icassp-skd/","section":"publication","summary":"","tags":[],"title":"On the Importance of Different Frequency Bins for Speaker Verification","type":"publication"},{"authors":["Bei Liu","Haoyu Wang","Zhengyang Chen","Shuai Wang","Yanmin Qian"],"categories":["SEL"],"content":"","date":1652451239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652451239,"objectID":"4e64fa64d73ad296e7315c30d4600565","permalink":"https://wsstriving.github.io/publication/2022-icassp-feats/","publishdate":"2022-05-13T22:13:59+08:00","relpermalink":"/publication/2022-icassp-feats/","section":"publication","summary":"","tags":[],"title":"Self-Knowledge Distillation via Feature Enhancement for Speaker Verification","type":"publication"},{"authors":["Hongji Wang","Chengdong Liang","Shuai Wang","Zhengyang Chen","Binbin Zhang","Xu Xiang","Yanlei Deng","Yanmin Qian"],"categories":["SEL"],"content":"","date":1652019239,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652019239,"objectID":"e743d44012471a12e8219790316f744a","permalink":"https://wsstriving.github.io/publication/2023-icassp-wespeaker/","publishdate":"2023-05-08T22:13:59+08:00","relpermalink":"/publication/2023-icassp-wespeaker/","section":"publication","summary":"","tags":[],"title":"Wespeaker: A Research and Production oriented Speaker Embedding Learning Toolkit","type":"publication"},{"authors":["Yufei Liu","Chengzhu Yu","Shuai Wang","Zhenchuan Yang","Chao Yang","Weibin Zhang"],"categories":["vc"],"content":"","date":1632832926,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632832926,"objectID":"bcd572179fb1eb1cfe297ed0d0224228","permalink":"https://wsstriving.github.io/publication/2021-is-vc/","publishdate":"2021-09-28T20:42:06+08:00","relpermalink":"/publication/2021-is-vc/","section":"publication","summary":"We introduce the speaker modeling method (statistics based) into the voice conversion","tags":[],"title":"Non-Parallel Any-to-Many Voice Conversion by Replacing Speaker Statistics","type":"publication"},{"authors":["Heinrich Dinkel","Shuai Wang","Xuenan Xu","Mengyue Wu","Kai Yu"],"categories":[],"content":"","date":1623593160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623593160,"objectID":"8437836abc13fc37c866bd0c83b93e27","permalink":"https://wsstriving.github.io/publication/2021-taslp-henri/","publishdate":"2021-06-13T22:06:00+08:00","relpermalink":"/publication/2021-taslp-henri/","section":"publication","summary":"Leaveraging weak-labeled data for voice activity detection in a teacher-student manner","tags":[],"title":"Voice activity detection in the wild: A data-driven approach using teacher-student training","type":"publication"},{"authors":["Xun Gong","Zhengyang Chen","Yexin Yang","Shuai Wang","Lan Wang","Yanmin Qian"],"categories":[],"content":"","date":1613226180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226180,"objectID":"00485dcf1dfbbe5f9def33d7c7845a11","permalink":"https://wsstriving.github.io/publication/2021-iscslp-ndm/","publishdate":"2021-02-13T22:23:00+08:00","relpermalink":"/publication/2021-iscslp-ndm/","section":"publication","summary":"","tags":[],"title":"Speaker Embedding Augmentation with Noise Distribution Matching.","type":"publication"},{"authors":["Shuai Wang","Yexin Yang","Yanmin Qian","Kai Yu"],"categories":[],"content":"","date":1613226168,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226168,"objectID":"b4a4d15ae5f125b83a1374564adce6c3","permalink":"https://wsstriving.github.io/publication/2021-iscslp-pooling/","publishdate":"2021-02-13T22:22:48+08:00","relpermalink":"/publication/2021-iscslp-pooling/","section":"publication","summary":"","tags":[],"title":"Revisiting the Statistics Pooling Layer in Deep Speaker Embedding Learning","type":"publication"},{"authors":["Zhengyang Chen","Shuai Wang","Yanmin Qian"],"categories":[],"content":"","date":1613225647,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613225647,"objectID":"efc4863f82076011334cec88602c0c72","permalink":"https://wsstriving.github.io/publication/2021-icassp-unsupervised/","publishdate":"2021-02-13T22:14:07+08:00","relpermalink":"/publication/2021-icassp-unsupervised/","section":"publication","summary":"","tags":[],"title":"SELF-SUPERVISED LEARNING BASED DOMAIN ADAPTATION FOR ROBUST SPEAKER VERIFICATION","type":"publication"},{"authors":["Chenpeng Du","Bing Han","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["data_aug"],"content":"","date":1613225641,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613225641,"objectID":"afb3b01a439951f63e5b037db99876d3","permalink":"https://wsstriving.github.io/publication/2021-icassp-ttsaug2/","publishdate":"2021-02-13T22:14:01+08:00","relpermalink":"/publication/2021-icassp-ttsaug2/","section":"publication","summary":"","tags":[],"title":"SYNAUG:SYNTHESIS-BASED DATA AUGMENTATION FOR TEXT-DEPENDENT SPEAKER VERIFICATION","type":"publication"},{"authors":["Houjun Huang","Xu Xiang","Fei Zhao","Shuai Wang","Yanmin Qian"],"categories":[],"content":"","date":1613225639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613225639,"objectID":"91807904facde6e1518f8593be74fef0","permalink":"https://wsstriving.github.io/publication/2021-icassp-ttsaug1/","publishdate":"2021-02-13T22:13:59+08:00","relpermalink":"/publication/2021-icassp-ttsaug1/","section":"publication","summary":"","tags":[],"title":"Unit Selection Synthesis based Data Augmentation for Fixed Phrase Speaker Verification","type":"publication"},{"authors":["Yanmin Qian","Zhengyang Chen","Shuai Wang"],"categories":[],"content":"","date":1613225160,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613225160,"objectID":"32d9c08f198b5be3917946e11bff87d9","permalink":"https://wsstriving.github.io/publication/2021-taslp/","publishdate":"2021-02-13T22:06:00+08:00","relpermalink":"/publication/2021-taslp/","section":"publication","summary":"An investigation of combining audio and visual information for person identity verification","tags":[],"title":"Audio-Visual Deep Neural Network for Robust Person Verification","type":"publication"},{"authors":["Shuai Wang","Yexin Yang","Zhanghao Wu","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1596120402,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596120402,"objectID":"0c5c2673b41b748369c1d2ff1ddc7069","permalink":"https://wsstriving.github.io/publication/2020-taslp/","publishdate":"2020-07-30T22:46:42+08:00","relpermalink":"/publication/2020-taslp/","section":"publication","summary":"Data augmentation is an effective method to improve the robustness of embedding based speaker verification systems, which could be applied to either the front-end speaker embedding extractor or the back-end PLDA. Different from the conventional augmentation methods such as manually adding noise or reverberation to the original audios, in this article, we propose to use deep generative models to directly generate more diverse speaker embeddings, which would be used for robust PLDA training. Conditional GAN, and VAE are designed, and investigated for different embedding types, including factor analysis based i-vector, TDNN based x-vector, and ResNet based r-vector. The proposed back-end augmentation methods are evaluated on NIST SRE 2016, and 2018 dataset. Within the popular x-vector, and r-vector framework, the experimental results show that our proposed methods can outperform the traditional audio based back-end augmentation method while different front-end augmentation methods are considered.","tags":[],"title":"Data Augmentation using Deep Generative Models for Embedding based Speaker Recognition","type":"publication"},{"authors":["Hongji Wang","Heinrich Dinkel","Shuai Wang","Yanmin Qian and Kai Yu"],"categories":[],"content":"","date":1595940126,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595940126,"objectID":"90d7b8622c75f523c4cf63d4b6471c32","permalink":"https://wsstriving.github.io/publication/2020-is-spoof/","publishdate":"2020-07-28T20:42:06+08:00","relpermalink":"/publication/2020-is-spoof/","section":"publication","summary":"INTERSPEECH 2020","tags":[],"title":"Dual-adversarial domain adaptation for generalized replay attack detection","type":"publication"},{"authors":["Zhengyang Chen","Shuai Wang and Yanmin Qian"],"categories":[],"content":"","date":1595940102,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595940102,"objectID":"3cf8a0ebd8293549a386c1b3ae87588e","permalink":"https://wsstriving.github.io/publication/2020-is-avn/","publishdate":"2020-07-28T20:41:42+08:00","relpermalink":"/publication/2020-is-avn/","section":"publication","summary":"INTERSPEECH 2020","tags":[],"title":"Multi-modality Matters: A Performance Leap on VoxCeleb","type":"publication"},{"authors":["Zhengyang Chen","Shuai Wang and Yanmin Qian"],"categories":[],"content":"","date":1595940096,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595940096,"objectID":"97633af1935d50d4ff181ebde80187f9","permalink":"https://wsstriving.github.io/publication/2020-is-psn/","publishdate":"2020-07-28T20:41:36+08:00","relpermalink":"/publication/2020-is-psn/","section":"publication","summary":"INTERSPEECH 2020","tags":[],"title":"Adversarial Domain Adaptation for Speaker Verification using Partially Shared Network","type":"publication"},{"authors":["Jahangir Alam","Gilles Boulianne","Lukáš Burget","Mohamed Dahmane","Mireia Diez","Ondrej Glembek","Marc Lalonde","Alicia Lozano-Diez","Pavel Matejka","Petr Mizera","Ladislav Mošner","Cédric Noiseux","Joao Monteiro","Ondrej Novotný","Oldrich Plchot","Johan Rohdin","Anna Silnova","Josef Slavıcek","Themos Stafylakis","Pierre-Luc St-Charles","Shuai Wang","Hossein Zeinali (alphabetical order)"],"categories":[],"content":"","date":1593150056,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593150056,"objectID":"5705d76ac7f3ffb2be00aa445bdf8f3e","permalink":"https://wsstriving.github.io/publication/2020-nistsre/","publishdate":"2020-06-26T13:40:56+08:00","relpermalink":"/publication/2020-nistsre/","section":"publication","summary":"We present a condensed description and analysis of the joint submission of ABC team for NIST SRE 2019, by BUT, CRIM, Phonexia, Omilia and UAM. We concentrate on challenges that arose during development and we analyze the results obtained on the evaluation data and on our development sets. The conversational telephone speech (CMN2) condition is challenging for current state-of-the-art systems, mainly due to the language mismatch between training and test data. We show that a combination of adversarial domain adaptation, backend adaptation and score normalization can mitigate this mismatch. On the VAST condition, we demonstrate the importance of deploying diarization when dealing with multi-speaker utterances and the drastic improvements that can be obtained by combining audio and visual modalities.","tags":[],"title":"Analysis of ABC Submission to NIST SRE 2019 CMN and VAST Challenge","type":"publication"},{"authors":["Federico Landini","Shuai Wang","Mireia Diez","Lukáš Burget","Pavel Matějka","Kateřina Žmolíková","Ladislav Mošner","Anna Silnova","Oldřich Plchot","Ondřej Novotný","Hossein Zeinali","Johan Rohdin"],"categories":["diarization"],"content":"","date":1593092762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092762,"objectID":"236062526873f7594d03148245c4040b","permalink":"https://wsstriving.github.io/publication/2020-icassp-dihard/","publishdate":"2020-06-25T21:46:02+08:00","relpermalink":"/publication/2020-icassp-dihard/","section":"publication","summary":"This paper describes the winning systems developed by the BUT team for the four tracks of the Second DIHARD Speech Diarization Challenge, with source code available","tags":[],"title":"But System for the Second Dihard Speech Diarization Challenge","type":"publication"},{"authors":["Mireia Diez","Lukáš Burget","Federico Landini","Shuai Wang","Honza Černocký"],"categories":["diarization"],"content":"","date":1593092757,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092757,"objectID":"e731bd48fbdbc402ae4c4569bca9bfc6","permalink":"https://wsstriving.github.io/publication/2020-icassp-vb/","publishdate":"2020-06-25T21:45:57+08:00","relpermalink":"/publication/2020-icassp-vb/","section":"publication","summary":"This paper presents an analysis of our diarization system winning the second DIHARD speech diarization challenge, track 1. This system is based on clustering x-vector speaker embeddings extracted every 0.25s from short segments of the input recording. In this paper, we focus on the two x-vector clustering methods employed, namely Agglomerative Hierarchical Clustering followed by a clustering based on Bayesian Hidden Markov Model (BHMM). Even though the system submitted to the challenge had further post-processing steps, we will show that using this BHMM solely is enough to achieve the best performance in the challenge. The analysis will show improvements achieved by optimizing individual processing steps, including a simple procedure to effectively perform domain adaptation by Probabilistic Linear Discriminant Analysis model interpolation. All experiments are performed in the DIHARD II evaluation framework.","tags":[],"title":"Optimizing Bayesian HMM based x-vector clustering for the second DIHARD speech diarization challenge","type":"publication"},{"authors":["Yexin Yang*","Shuai Wang*","Xun Gong","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1593092752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092752,"objectID":"31049aaa0b60059d482c4bd57ff85269","permalink":"https://wsstriving.github.io/publication/2020-icassp-adapt/","publishdate":"2020-06-25T21:45:52+08:00","relpermalink":"/publication/2020-icassp-adapt/","section":"publication","summary":"We proposed the text-adptation speaker verification task and an intital solution called Speaker-text factorization network, which could deal with different text-mismatch conditions","tags":[],"title":"Text Adaptation for Speaker Verification with Speaker-Text Factorized Embeddings","type":"publication"},{"authors":["Zhengyang Chen","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1593092745,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092745,"objectID":"233e9b343cd1a58ae192670662893b4f","permalink":"https://wsstriving.github.io/publication/2020-icassp-channel/","publishdate":"2020-06-25T21:45:45+08:00","relpermalink":"/publication/2020-icassp-channel/","section":"publication","summary":"Using deep neural network to extract speaker embedding has significantly improved the speaker verification task. However, such embeddings are still vulnerable to channel variability. Previous works have used adversarial training to suppress channel information to extract channel-invariant embedding and achieved a significant improvement. Inspired by the successful joint multi-task and adversarial training with phonetic information for phonetic-invariant speaker embedding learning, in this paper, a similar methodology is developed to suppress the channel variability. By treating the recording devices or environments as the channel variability, two individual experiments are carried out, and consistent performance improvement is observed in both cases. The best performance is obtained by sequentially applying multi-task training at the statistics pooling layer and adversarial training at the embedding layer, achieving 10.77% and 9.37% relative improvements in terms of EER compared to the baselines, for the recording environments or devices level, respectively","tags":[],"title":"Channel Invariant Speaker Embedding Learning with Joint Multi-Task and Adversarial Training","type":"publication"},{"authors":["Shuai Wang","Johan Rohdin","Oldřich Plchot","Lukáš Burget","Kai Yu","Jan Černocký"],"categories":["sid","data_aug"],"content":"","date":1593092736,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092736,"objectID":"dc85fba0891293c64389195f152a5fa2","permalink":"https://wsstriving.github.io/publication/2020-icassp-specaug/","publishdate":"2020-06-25T21:45:36+08:00","relpermalink":"/publication/2020-icassp-specaug/","section":"publication","summary":"SpecAugment is a newly proposed data augmentation method for speech recognition. By randomly masking bands in the log Mel spectogram this method leads to impressive performance improvements. In this paper, we investigate the usage of SpecAugment for speaker verification tasks. Two different models, namely 1-D convolutional TDNN and 2-D convolutional ResNet34, trained with either Softmax or AAM-Softmax loss, are used to analyze SpecAugment’s effectiveness. Experiments are carried out on the Voxceleb and NIST SRE 2016 dataset. By applying SpecAugment to the original clean data in an on-the-fly manner without complex off-line data augmentation methods, we obtained 3.72% and 11.49% EER for NIST SRE 2016 Cantonese and Tagalog, respectively. For Voxceleb1 evaluation set, we obtained 1.47% EER.","tags":[],"title":"Investigation of Specaugment for Deep Speaker Embedding Learning","type":"publication"},{"authors":["Hossein Zeinali","Shuai Wang","Anna Silnova","Pavel Matějka","Oldřich Plchot"],"categories":["SEL"],"content":"","date":1561527079,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561527079,"objectID":"2d0e1a86fc213954acae3bb0fa72c3e0","permalink":"https://wsstriving.github.io/publication/2019-voxceleb/","publishdate":"2020-06-26T13:31:19+08:00","relpermalink":"/publication/2019-voxceleb/","section":"publication","summary":"This paper describes the winning systems developed by the BUT team for the two tracks of the First VoxSRC Speaker Recognition Challenge, we proposed r-vector in this paper. Update: I lanuched an open-source project wespeaker, where the implementation can be found","tags":[],"title":"BUT system description to voxceleb speaker recognition challenge 2019","type":"publication"},{"authors":["Shuai Wang","Zili Huang","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1561478166,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561478166,"objectID":"b1ca2a2551616884eacb8c92571fcd20","permalink":"https://wsstriving.github.io/publication/2019-taslp/","publishdate":"2020-06-25T23:56:06+08:00","relpermalink":"/publication/2019-taslp/","section":"publication","summary":"Short duration text-independent speaker verification remains a hot research topic in recent years, and deep neural network based embeddings have shown impressive results in such conditions. Good speaker embeddings require the property of both small intra-class variation and large inter-class difference, which is critical for the ability of discrimination and generalization. Current embedding learning strategies can be grouped into two frameworks: “Cascade embedding learning” with multiple stages and “direct embedding learning” from spectral feature directly. We propose new approaches to achieve more discriminant speaker embeddings. Within the cascade framework, a neural network based deep discriminant analysis (DDA) is proposed to project i-vector to more discriminative embeddings. Within the direct embedding framework, a deep model with more advanced center loss and A-softmax loss is used, the focal loss is also investigated in this framework. Moreover, the traditional i-vector and neural embeddings are finally combined with neural network based DDA to achieve further gain. Main experiments are carried out on a short-duration text-independent speaker verification dataset generated from the SRE corpus. The results show that the newly proposed method is promising for short-duration text-independent speaker verification, and it is consistently better than traditional i-vector and neural embedding baselines. The best embeddings achieve roughly 30% relative EER reduction compared to the i-vector baseline, which could be further enhanced when combined with the i-vector system.","tags":[],"title":"Discriminative Neural Embedding Learning for Short-Duration Text-Independent Speaker Verification","type":"publication"},{"authors":["Xu Xiang","Shuai Wang","Houjun Huang","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1561476404,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561476404,"objectID":"a5b70cd53ce0afb2c1e6bd84b6a302c5","permalink":"https://wsstriving.github.io/publication/2019-apsipa/","publishdate":"2020-06-25T23:26:44+08:00","relpermalink":"/publication/2019-apsipa/","section":"publication","summary":"Recently, speaker embeddings extracted from a speaker discriminative deep neural network (DNN) yield better performance than the conventional methods such as i-vector. In most cases, the DNN speaker classifier is trained using cross entropy loss with softmax. However, this kind of loss function does not explicitly encourage inter-class separability and intra-class compactness. As a result, the embeddings are not optimal for speaker recognition tasks. In this paper, to address this issue, three different margin based losses which not only separate classes but also demand a fixed margin between classes are introduced to deep speaker embedding learning. It could be demonstrated that the margin is the key to obtain more discriminative speaker embeddings. Experiments are conducted on two public text independent tasks: VoxCeleb1 and Speaker in The Wild (SITW). The proposed approach can achieve the state-ofthe-art performance, with 25% ∼ 30% equal error rate (EER) reduction on both tasks when compared to strong baselines using cross entropy loss with softmax, obtaining 2.238% EER on VoxCeleb1 test set and 2.761% EER on SITW core-core test set, respectively","tags":[],"title":"Margin matters: Towards more discriminative deep neural network embeddings for speaker recognition","type":"publication"},{"authors":["Shuai Wang","Yexin Yang","Tianzhe Wang","Yanmin Qian and Kai Yu"],"categories":["SEL"],"content":"","date":1561476194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561476194,"objectID":"ee505171a7dca598cf8a5d67bdfc114b","permalink":"https://wsstriving.github.io/publication/2019-icassp/","publishdate":"2020-06-25T23:23:14+08:00","relpermalink":"/publication/2019-icassp/","section":"publication","summary":"Deep speaker embedding learning is an effective method for speaker identity modelling. Very deep models such as ResNet can achieve remarkable results but are usually too computationally expensive for real applications with limited resources. On the other hand, simply reducing model size is likely to result in significant performance degradation. In this paper, label-level and embedding-level knowledge distillation are proposed to narrow down the performance gap between large and small models. Label-level distillation utilizes the posteriors obtained by a well-trained teacher model to guide the opti-mization of the student model, while embedding-level distillation directly constrains the similarity between embeddings learned by two models. Experiments were carried out on the VoxCeleb1 dataset. Results show that the proposed knowledge distillation methods can significantly boost the performance of highly compact student models","tags":[],"title":"Knowledge Distillation for Small Foot-print Deep Speaker Embedding","type":"publication"},{"authors":["Yefei Chen","Shuai Wang","Yanmin Qian and Kai Yu"],"categories":["vad"],"content":"","date":1561476028,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561476028,"objectID":"d9c6670a7d8c74af619d17d99815e86a","permalink":"https://wsstriving.github.io/publication/2019-ncmmsc-vad/","publishdate":"2020-06-25T23:20:28+08:00","relpermalink":"/publication/2019-ncmmsc-vad/","section":"publication","summary":"Voice activity detection (VAD) is an essential pre-processing step for tasks such as automatic speech recognition (ASR) and speaker recognition. A basic goal is to remove silent segments within an audio, while a more general VAD system could remove all the irrelevant segments such as noise and even unwanted speech from non-target speakers. We define the task, which only detects the speech from the target speaker, as speaker-dependent voice activity detection (SDVAD). This task is quite common in real applications and usually implemented by performing speaker verification (SV) on audio segments extracted from VAD. In this paper, we propose an end-to-end neural network based approach to address this problem, which explicitly takes the speaker identity into the modeling process. Moreover, inference can be performed in an online fashion, which leads to low system latency. Experiments are carried out on a conversational telephone dataset generated from the Switchboard corpus. Results show that our proposed online approach achieves significantly better performance than the usual VAD/SV system in terms of both frame accuracy and F-score. We also used our previously proposed segment-level metric for a more comprehensive analysis.","tags":[],"title":"End-to-End Speaker-Dependent Voice Activity Detection","type":"publication"},{"authors":["Yexin Yang","Hongji Wang","Heinrich Dinkel","Zhengyang Chen","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["antispoof"],"content":"","date":1561473614,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473614,"objectID":"145c2679ec539a04780d22bca57b6213","permalink":"https://wsstriving.github.io/publication/2019-is-asvspoof/","publishdate":"2020-06-25T22:40:14+08:00","relpermalink":"/publication/2019-is-asvspoof/","section":"publication","summary":"The robustness of an anti-spoofing system is progressively more important in order to develop a reliable speaker verification system. Previous challenges and datasets mainly focus on a specific type of spoofing attacks. The ASVspoof 2019 edition is the first challenge to address two major spoofing types-logical and physical access. This paper presents the SJTU’s submitted antispoofing system to the ASVspoof 2019 challenge. Log-CQT features are developed in conjunction with multi-layer convolutional neural networks for robust performance across both subtasks. CNNs with gradient linear units (GLU) activations are utilized for spoofing detection. The proposed system shows consistent performance improvement over all types of spoofing attacks. Our primary submissions achieve the 5th and 8th positions for the logical and physical access respectively. Moreover, our contrastive submission to the PA task exhibits","tags":[],"title":"The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge","type":"publication"},{"authors":["Mireia Diez","Lukáš Burget","Shuai Wang","Johan Rohdin","Jan Černocký"],"categories":["diarization"],"content":"","date":1561473597,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473597,"objectID":"94309524df07368a9a0d3b263cdfd082","permalink":"https://wsstriving.github.io/publication/2019-is-vb/","publishdate":"2020-06-25T22:39:57+08:00","relpermalink":"/publication/2019-is-vb/","section":"publication","summary":"This paper presents a simplified version of the previously proposed diarization algorithm based on Bayesian Hidden Markov Models, which uses Variational Bayesian inference for very fast and robust clustering of x-vector (neural network based speaker embeddings). The presented results show that this clustering algorithm provides significant improvements in diarization performance as compared to the previously used Agglomerative Hierarchical Clustering. The output of this system can be further employed as an initialization for a second stage VB diarization system, using frame-wise MFCC features as input, to obtain optimal results.","tags":[],"title":"Bayesian HMM Based x-Vector Clustering for Speaker Diarization","type":"publication"},{"authors":["Hongji Wang","Heinrich Dinkel","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["antispoof"],"content":"","date":1561473594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473594,"objectID":"ffde9d6e4c496b23dd5f1313152cdf37","permalink":"https://wsstriving.github.io/publication/2019-is-spoof/","publishdate":"2020-06-25T22:39:54+08:00","relpermalink":"/publication/2019-is-spoof/","section":"publication","summary":"Replay spoofing attacks are a major threat for speaker verification systems. Although many anti-spoofing systems or countermeasures are proposed to detect dataset-specific replay attacks with promising performance, they generalize poorly when applied on unseen datasets. In this work, the cross-dataset scenario is treated as a domain-mismatch problem and dealt with using a domain adversarial training framework. Compared with previous approaches, features learned from this newly-designed architecture are more discriminative for spoofing detection, but more indistinguishable across different domains. Only labeled source-domain data and unlabeled target-domain data are required during the adversarial training process, which can be regarded as unsupervised domain adaptation. Experiments on the ASVspoof 2017 V. 2 dataset as well as the physical access condition part of BTAS 2016 dataset demonstrate that a significant EER reduction of over relative 30% can be obtained after applying the proposed domain adversarial training framework. It is shown that our proposed model can benefit from a large amount of unlabeled target-domain training data to improve detection accuracy.","tags":[],"title":"Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training","type":"publication"},{"authors":["Zhanghao Wu","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1561473566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473566,"objectID":"cf315e8ccba60599b01533a44842ebc2","permalink":"https://wsstriving.github.io/publication/2019-is-vae/","publishdate":"2020-06-25T22:39:26+08:00","relpermalink":"/publication/2019-is-vae/","section":"publication","summary":"Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and x-vector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54% and 7.84%, and a more significant improvement for x-vector on those two languages with EERs of 11.86% and 4.20%.","tags":[],"title":"Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification","type":"publication"},{"authors":["Shuai Wang","Johan Rohdin","Lukáš Burget","Oldřich Plchot","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1561473531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473531,"objectID":"9bebd5e034e075bbcdae4bdd320fd095","permalink":"https://wsstriving.github.io/publication/2019-is-phoneme/","publishdate":"2020-06-25T22:38:51+08:00","relpermalink":"/publication/2019-is-phoneme/","section":"publication","summary":"We proposed the segment-level representation for phonetic information and the corresponding segment-level multi-task/adversarial training framework, we revisited the usage the phonetic information for the text-independent embedding learning and designed experiments to verify the assumption: For TI-SV, it could be benificial to remove the phonetic variation in the final speaker embeddings","tags":[],"title":"On the Usage of Phonetic Information for Text-independent Speaker Embedding Extraction","type":"publication"},{"authors":["Yanmin Qian","Chao Weng","Xuankai Chang","Shuai Wang and Dong Yu"],"categories":[],"content":"","date":1529941802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529941802,"objectID":"38b89a990a96c003d43c7afca5806cc9","permalink":"https://wsstriving.github.io/publication/2018-fitee/","publishdate":"2020-06-25T23:50:02+08:00","relpermalink":"/publication/2018-fitee/","section":"publication","summary":"The cocktail party problem, i.e., tracing and recognizing the speech of a specific speaker when multiple speakers talk simultaneously, is one of the critical problems yet to be solved to enable the wide application of automatic speech recognition (ASR) systems. In this overview paper, we review the techniques proposed in the last two decades in attacking this problem. We focus our discussions on the speech separation problem given its central role in the cocktail party environment, and describe the conventional single-channel techniques such as computational auditory scene analysis (CASA), non-negative matrix factorization (NMF) and generative models, the conventional multi-channel techniques such as beamforming and multi-channel blind source separation, and the newly developed deep learning-based techniques, such as deep clustering (DPCL), the deep attractor network (DANet), and permutation invariant training (PIT). We also present techniques developed to improve ASR accuracy and speaker identification in the cocktail party environment. We argue effectively exploiting information in the microphone array, the acoustic training set, and the language itself using a more powerful model. Better optimization objective and techniques will be the approach to solving the cocktail party problem.","tags":[],"title":"Past review, current progress, and challenges ahead on the cocktail party problem","type":"publication"},{"authors":["Zili Huang*","Shuai Wang*","Kai Yu"],"categories":["SEL"],"content":"","date":1529940743,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529940743,"objectID":"32f57953d4c4cabdd8ba7e043e77fd23","permalink":"https://wsstriving.github.io/publication/2018-interspeech/","publishdate":"2020-06-25T23:32:23+08:00","relpermalink":"/publication/2018-interspeech/","section":"publication","summary":"Recently, researchers propose to build deep learning based endto-end speaker verification (SV) systems and achieve competitive results compared with the standard i-vector approach. In addition to deep learning architectures, optimization metric such as softmax loss or triplet loss, is important for extracting speaker embeddings which are discriminative and generalizable to unseen speakers. In this paper, angular softmax (A-softmax) loss is introduced to improve speaker embedding quality. It is investigated in two SV frameworks: a CNN based end-toend SV framework and an i-vector SV framework where deep discriminant analysis is used for channel compensation. Experimental results on a short-duration text-independent speaker verification dataset generated from SRE reveal that A-softmax achieves significant performance improvement compared with other metrics in both frameworks.","tags":[],"title":"Angular Softmax for Short-Duration Text-independent Speaker Verification.","type":"publication"},{"authors":["Shuai Wang","Heinrich Dinkel","Yanmin Qian","Kai Yu"],"categories":[],"content":"","date":1529940736,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529940736,"objectID":"152afeffd51f93a785ed060ea4955010","permalink":"https://wsstriving.github.io/publication/2018-iscide/","publishdate":"2020-06-25T23:32:16+08:00","relpermalink":"/publication/2018-iscide/","section":"publication","summary":"d-vector approach achieved impressive results in speaker verification. Representation is obtained at utterance level by calculating the mean of the frame level outputs of a hidden layer of the DNN. Although mean based speaker identity representation has achieved good performance, it ignores the variability of frames across the whole utterance, which consequently leads to information loss. This is particularly serious for text-dependent speaker verification, where within-utterance feature variability better reflects text variability than the mean. To address this issue, a new covariance based speaker representation is proposed in this paper. Here, covariance of the frame level outputs is calculated and incorporated into the speaker identity representation. The proposed approach is investigated within a joint multi-task learning framework for text-dependent speaker verification. Experiments on RSR2015 and RedDots showed that, covariance based deep feature can significantly improve the performance compared to the traditional mean based deep features.","tags":[],"title":"Covariance based deep feature for text-dependent speaker verification","type":"publication"},{"authors":["Shuai Wang","Zili Huang","Yanmin Qian and Kai Yu"],"categories":["SEL"],"content":"","date":1529940727,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529940727,"objectID":"52dc35bfc14177fe3053a859e5fef771","permalink":"https://wsstriving.github.io/publication/2018-iscslp-dda/","publishdate":"2018-06-25T23:32:07+08:00","relpermalink":"/publication/2018-iscslp-dda/","section":"publication","summary":"Linear Discriminant Analysis (LDA) has been used as a standard post-processing procedure in many state-of-the-art speaker recognition tasks. Through maximizing the inter-speaker difference and minimizing the intra-speaker variation, LDA projects i-vectors to a lower-dimensional and more discriminative subspace. In this paper, we propose a neural network based compensation scheme(termed as deep discriminant analysis, DDA) for i-vector based speaker recognition, which shares the idea with LDA. Optimized against softmax loss and center loss at the same time, the proposed method learns a more compact and discriminative embedding space. Compared with the Gaussian distribution assumption of data and the learned linear projection in LDA, the proposed method doesn't pose any assumptions on data and can learn a non-linear projection function. Experiments are carried out on a short-duration text-independent dataset based on the SRE Corpus, noticeable performance improvement can be observed against the normal LDA or PLDA methods.","tags":[],"title":"Deep discriminant analysis for i-vector based robust speaker recognition","type":"publication"},{"authors":["Yexin Yang","Shuai Wang","Man Sun","Yanmin Qian","Kai Yu"],"categories":[],"content":"","date":1529940722,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529940722,"objectID":"fde67b190b0657c3908e9b9e9579b760","permalink":"https://wsstriving.github.io/publication/2018-iscslp-gan/","publishdate":"2020-06-25T23:32:02+08:00","relpermalink":"/publication/2018-iscslp-gan/","section":"publication","summary":"Data augmentation is an effective method to increase the quantity of training data, which improves the model's robustness and generalization ability. In this paper, we propose a generative adversarial network (GAN) based data augmentation approach for probabilistic linear discriminant analysis (PLDA), which is a standard back-end for state-of-the-art x-vector based speaker verification system. Instead of generating new spectral feature samples, a conditional Wasserstein GAN is adopted to directly generate x-vectors. Experiments are carried out on the standard NIST SRE 2016 evaluation dataset. Compared to manually adding noise, the GAN augmented PLDA achieves better performance and this performance can be further boosted when combined with manual augmented data. EER of 11.68% and 4.43% were obtained for Tagalog and Cantonese evaluation condition, respectively.","tags":[],"title":"Generative Adversarial Networks based X-vector Augmentation for Robust Probabilistic Linear Discriminant Analysis in Speaker Verification","type":"publication"},{"authors":["Shuai Wang","Yanmin Qian","Kai Yu"],"categories":[],"content":"","date":1529940607,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529940607,"objectID":"8cf36aa5ba9a09525490d9249804bfcc","permalink":"https://wsstriving.github.io/publication/2018-icassp-cocktail/","publishdate":"2020-06-25T23:30:07+08:00","relpermalink":"/publication/2018-icassp-cocktail/","section":"publication","summary":"Recognizing the identities of multiple talkers via their overlapped speech is a challenging task, it is also one main difficulty for the “cocktail party problem”. In this paper, a novel dilated convolutional neural network with a focal KL-divergence loss function is proposed to tackle this problem. During training, relative loss for the well-classified samples is automatically reduced and consequently more attention is paid to the hard samples. The use of the focal KL-divergence loss function leads to more stable training and improved testing performance. Furthermore, a post processing of assigning different frames with different weights is also adopted and leads to further improvement. The proposed framework can be easily extended from 2-talker to 3-talker speaker identification scenario. Experiments on the artificially generated RSR2015 multi-talker mixed corpus show that the proposed approach can improve multi-talker speaker identification significantly.","tags":[],"title":"Focal KL-divergence based dilated convolutional neural networks for co-channel speaker identification","type":"publication"},{"authors":["Zili Huang","Shuai Wang","Yanmin Qian"],"categories":[],"content":"","date":1529940599,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529940599,"objectID":"418131f17fc7dedcfca994f29abef8a1","permalink":"https://wsstriving.github.io/publication/2018-icassp-joint/","publishdate":"2020-06-25T23:29:59+08:00","relpermalink":"/publication/2018-icassp-joint/","section":"publication","summary":"Factor analysis based i-vector has been the state-of-the-art method for speaker verification. Recently, researchers propose to build DNN based end-to-end speaker verification systems and achieve comparable performance with i -vector. Since these two methods possess their own property and differ from each other significantly, we explore a framework to integrate these two paradigms together to utilize their complementarity. More specifically, in this paper we develop and compare four methodologies to integrate traditional i -vector into end-to-end systems, including score fusion, embeddings concatenation, transformed concatenation and joint learning. All these approaches achieve significant gains. Moreover, the hard trial selection is performed on the end-to-end architecture which further improves the performance. Experimental results on a text-independent short-duration dataset generated from SRE 2010 reveal that the newly proposed method reduces the EER by relative 31.0% and 28.2% compared to the i-vector and end-to-end baselines respectively.","tags":[],"title":"Joint i-vector with end-to-end system for short-duration text-independent speaker verification","type":"publication"},{"authors":["Xiaowei Jiang","Shuai Wang","Xu Xiang","Yanmin Qian"],"categories":[],"content":"","date":1498405809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498405809,"objectID":"120e8b5a28faac1c555396fdbcacf386","permalink":"https://wsstriving.github.io/publication/2017-apsipa/","publishdate":"2020-06-25T23:50:09+08:00","relpermalink":"/publication/2017-apsipa/","section":"publication","summary":"GMM-UBM is widely used for the text-dependent task for its simplicity and effectiveness, while i-vector provides a compact representation for speaker information. Thus it is promising to fuse these two frameworks. In this paper, a variation of traditional i-vector extracted at frame level is appended with MFCC as tandem features. Incorporating this feature into GMM-UBM system achieves 26% and 41% performance gain compared with DNN i-vector baseline on the RSR2015 and RedDots evaluation set, respectively. Moreover, the performance of the proposed system that trained on 86 hours data is on par with that of the DNN i-vector baseline trained on a much larger dataset (5000 hours).","tags":[],"title":"Integrating Online i-vector into GMM-UBM for Text-dependent Speaker Verification","type":"publication"},{"authors":["Shuai Wang","Yanmin Qian and Kai Yu"],"categories":["SEL"],"content":"","date":1498384882,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498384882,"objectID":"49daf81e9b3979471a1960974d3c31d0","permalink":"https://wsstriving.github.io/publication/2017-interspeech/","publishdate":"2020-06-25T18:01:22+08:00","relpermalink":"/publication/2017-interspeech/","section":"publication","summary":"The first attempt to systematically analyze the information encoded in speaker embeddings (prior to x-vector), detailed analysis on x-vectors could be refered to the paper Probing the Information Encoded in X-vectors from JHU ","tags":[],"title":"What Does the Speaker Embedding Encode?","type":"publication"}]